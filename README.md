# ğŸ™ï¸ Transkriptor â€“ Lokale Spracherkennung mit KI-Zusammenfassungen

Eine vollstÃ¤ndig lokal gehostete Web-Anwendung zur automatischen Transkription von Audio- und Videodateien mit Sprecherzuweisung und intelligenten KI-Zusammenfassungen.

**ğŸš€ Neu:** Integriertes KI-System fÃ¼r automatische Zusammenfassungen mit 5 verschiedenen Typen â€“ vollstÃ¤ndig lokal und DSGVO-konform!

## âœ¨ Features

- **ğŸ¯ Lokale Verarbeitung** â€“ Deine Daten verlassen nie deinen Server
- **ğŸ‘¥ Sprecher-Diarisierung** â€“ Automatische Erkennung verschiedener Sprecher (bis zu 30 Sprecher)
- **ğŸŒ Multi-Sprache** â€“ UnterstÃ¼tzt 20+ Sprachen (Deutsch, Englisch, etc.)
- **â±ï¸ Zeitstempel** â€“ Wortgenaue oder Segment-Zeitstempel
- **âœï¸ Editor** â€“ Transkripte direkt im Browser bearbeiten
- **ğŸ¤– KI-Zusammenfassungen** â€“ 5 verschiedene Zusammenfassungstypen mit lokalem LLM (Ollama)
- **ğŸ—‚ï¸ Tab-Navigation** â€“ Ãœbersichtliche Trennung von Transkript und Zusammenfassungen
- **ğŸ’¾ Persistenz** â€“ Automatisches Speichern von Transkripten und Zusammenfassungen
- **ğŸ“ Export** â€“ TXT, SRT, VTT, JSON, Word-Format

## ğŸ–¥ï¸ Systemanforderungen

- **Docker** mit Docker Compose
- **NVIDIA GPU** mit mindestens 8 GB VRAM (fÃ¼r groÃŸe Modelle)
- **NVIDIA Container Toolkit** installiert
- **Hugging Face Account** (kostenlos)

### GPU Setup (falls noch nicht vorhanden)

```bash
# NVIDIA Container Toolkit installieren (Ubuntu/Debian)
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg

curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt update && sudo apt install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

## ğŸš€ Installation

### 1. Repository klonen/Dateien kopieren

```bash
mkdir whisper-transcriber && cd whisper-transcriber
# Alle Dateien in dieses Verzeichnis kopieren
```

### 2. Hugging Face Token einrichten

1. Erstelle einen kostenlosen Account auf [huggingface.co](https://huggingface.co)
2. Generiere einen **READ** Token unter: https://huggingface.co/settings/tokens
3. Akzeptiere die Nutzungsbedingungen fÃ¼r diese Modelle:
   - https://huggingface.co/pyannote/segmentation-3.0
   - https://huggingface.co/pyannote/speaker-diarization-3.1

### 3. Konfiguration erstellen

```bash
# .env Datei aus Vorlage erstellen
cp .env.example .env

# Token eintragen
nano .env
# Ersetze "hf_DEIN_TOKEN_HIER" mit deinem echten Token
```

### 4. Starten

```bash
docker compose up -d
```

â³ **Erster Start dauert lÃ¤nger** â€“ Die Modelle werden heruntergeladen (~10 GB fÃ¼r WhisperX).

### 5. Ollama LLM fÃ¼r Zusammenfassungen einrichten (Optional)

Das Ollama LLM fÃ¼r KI-Zusammenfassungen wird automatisch mit gestartet. Lade das empfohlene Modell herunter:

```bash
# Empfohlenes Modell (4.5 GB, benÃ¶tigt ~5 GB VRAM)
docker exec ollama ollama pull qwen2.5:7b

# Alternative fÃ¼r weniger VRAM (2 GB, benÃ¶tigt ~2 GB VRAM)
docker exec ollama ollama pull llama3.2:3b
```

**Hinweis:** WhisperX und Ollama teilen sich die GPU sequentiell. Erst wird transkribiert, dann kÃ¶nnen Zusammenfassungen generiert werden.

### 6. Ã–ffnen

Ã–ffne im Browser: **http://localhost:3000**

## ğŸ“‹ Nutzung

### Transkription erstellen

1. **Datei hochladen** â€“ Audio (MP3, WAV, M4A...) oder Video (MP4, MKV...)
2. **Optionen wÃ¤hlen**:
   - Sprache (automatisch oder manuell)
   - Min./Max. Anzahl Sprecher (1-30)
   - Sprecher-Erkennung ein/aus
   - Wort-Zeitstempel ein/aus
3. **Warten** â€“ Je nach DateigrÃ¶ÃŸe einige Sekunden bis Minuten
4. **Bearbeiten** â€“ Im **Transkript-Tab**:
   - Sprecher umbenennen
   - Text korrigieren
   - Segmente zusammenfÃ¼hren oder teilen
   - Bulk-Edit fÃ¼r mehrere Segmente
5. **Exportieren** â€“ Format wÃ¤hlen und herunterladen

### KI-Zusammenfassungen nutzen

Nach der Transkription wechselst du zum **Zusammenfassung-Tab**:

1. **Typ auswÃ¤hlen** aus dem Dropdown:
   - **Kurze Ãœbersicht** â€“ Executive Summary in 3-5 SÃ¤tzen
   - **Strukturierte Zusammenfassung** â€“ Mit Hauptthema, Kernpunkten und Fazit
   - **Zeitstempel-basiert** â€“ Chronologische Zusammenfassung mit Zeitmarken
   - **Action Items** â€“ Extrahierte Aufgaben als Checkliste
   - **Tags / Schlagworte** â€“ Relevante Themen und Schlagworte

2. **Generieren klicken** â€“ Die Zusammenfassung wird in Echtzeit gestreamt

3. **Zwischen Typen wechseln** â€“ Bereits generierte Zusammenfassungen werden sofort geladen, ohne neu zu generieren

4. **Exportieren oder Kopieren** â€“ Zusammenfassungen als TXT exportieren oder in Zwischenablage kopieren

**Hinweis:** Alle Zusammenfassungen werden automatisch gespeichert und sind auch nach einem Neuladen verfÃ¼gbar.

## ğŸ›ï¸ Modelle & Performance

### WhisperX ASR-Modelle

| Modell | VRAM | QualitÃ¤t | Geschwindigkeit |
|--------|------|----------|-----------------|
| `tiny` | ~1 GB | â­â­ | âš¡âš¡âš¡âš¡âš¡ |
| `base` | ~1 GB | â­â­â­ | âš¡âš¡âš¡âš¡ |
| `small` | ~2 GB | â­â­â­â­ | âš¡âš¡âš¡ |
| `medium` | ~5 GB | â­â­â­â­ | âš¡âš¡ |
| `large-v3` | ~10 GB | â­â­â­â­â­ | âš¡ |

Modell Ã¤ndern in `.env`:
```bash
ASR_MODEL=small  # oder: tiny, base, medium, large-v3
```

### Ollama LLM-Modelle (fÃ¼r Zusammenfassungen)

| Modell | VRAM | Download | Tokens/s | QualitÃ¤t |
|--------|------|----------|----------|----------|
| `llama3.2:3b` | ~2 GB | 2 GB | ~60 | Gut |
| `qwen2.5:7b` | ~5 GB | 4.5 GB | ~35 | â­ Exzellent (Empfohlen) |
| `llama3.1:8b` | ~5 GB | 4.7 GB | ~35 | Sehr gut |
| `mistral:7b` | ~4.5 GB | 4.1 GB | ~40 | Sehr gut |

**Empfehlung:** `qwen2.5:7b` â€“ Beste QualitÃ¤t fÃ¼r strukturierte Zusammenfassungen

Modell Ã¤ndern in `.env`:
```bash
OLLAMA_MODEL=qwen2.5:7b
```

Dann neues Modell herunterladen:
```bash
docker exec ollama ollama pull qwen2.5:7b
docker compose restart frontend
```

**GPU-Speicher:** WhisperX und Ollama teilen sich die GPU. Empfohlen sind mindestens 8 GB VRAM fÃ¼r beide Dienste.

## ğŸ”§ Konfiguration

### Umgebungsvariablen (.env)

```bash
# Hugging Face Token (ERFORDERLICH fÃ¼r Sprecher-Diarisierung)
HF_TOKEN=hf_deinTokenHier

# WhisperX ASR-Modell
ASR_MODEL=small  # tiny, base, small, medium, large-v3

# Ollama LLM-Modell fÃ¼r Zusammenfassungen
OLLAMA_MODEL=qwen2.5:7b  # qwen2.5:7b, llama3.1:8b, llama3.2:3b, mistral:7b
```

### docker-compose.yml Optionen

```yaml
environment:
  # ASR Engine (whisperx fÃ¼r Diarization)
  - ASR_ENGINE=whisperx

  # ModellgrÃ¶ÃŸe (von .env)
  - ASR_MODEL=${ASR_MODEL}

  # Hugging Face Token (fÃ¼r Sprecher-Erkennung)
  - HF_TOKEN=${HF_TOKEN}
```

### Ports Ã¤ndern

Frontend: Port `3000` â†’ z.B. `8080`:
```yaml
frontend:
  ports:
    - "8080:80"
```

Dienste-Ports:
- **Frontend:** 3000 (Web-UI)
- **API:** 9000 (WhisperX)
- **Ollama:** 11434 (LLM)

### Persistenz & Speicherung

Die Anwendung speichert automatisch:

- **Transkript-Daten** â†’ localStorage (~5-10 MB)
- **Audio-Dateien** â†’ IndexedDB (bis zu 50% freier Speicherplatz)
- **Zusammenfassungen** â†’ localStorage (alle Typen separat gespeichert)
- **Modell-Cache** â†’ Docker Volumes (`whisper_cache`, `ollama_models`)

**Retention:** 7 Tage automatische Aufbewahrung, danach werden Daten gelÃ¶scht.

## ğŸ› Troubleshooting

### "API offline"

```bash
# Logs prÃ¼fen
docker compose logs whisper-api

# Container neustarten
docker compose restart whisper-api
```

### GPU nicht erkannt

```bash
# NVIDIA Docker testen
docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi

# Falls Fehler: nvidia-container-toolkit neu konfigurieren
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

### Diarization funktioniert nicht

1. HF_TOKEN in `.env` korrekt eingetragen?
2. Nutzungsbedingungen der pyannote-Modelle akzeptiert?
3. Container neu starten nach Token-Ã„nderung

### Out of Memory (GPU)

Kleineres Modell verwenden:
```bash
# In .env Ã¤ndern:
ASR_MODEL=small  # statt large-v3
OLLAMA_MODEL=llama3.2:3b  # statt qwen2.5:7b

# Container neu starten
docker compose restart
```

**Tipp:** Ollama entlÃ¤dt Modelle nach 10 Minuten InaktivitÃ¤t automatisch, um VRAM freizugeben.

### Zusammenfassungen werden nicht generiert

```bash
# PrÃ¼fe ob Ollama lÃ¤uft
docker compose ps ollama

# PrÃ¼fe ob Modell heruntergeladen wurde
docker exec ollama ollama list

# Falls nicht, Modell herunterladen
docker exec ollama ollama pull qwen2.5:7b

# Container neu starten
docker compose restart ollama frontend
```

### Zusammenfassungen sind langsam

- **Erstes Request ist langsamer** (Model Loading ~5-10 Sekunden)
- **Kleineres Modell nutzen:** `llama3.2:3b` ist 2x schneller als `qwen2.5:7b`
- **GPU-Nutzung prÃ¼fen:** `docker exec ollama nvidia-smi`

## ğŸ“ Verzeichnisstruktur

```
whisper-transcriber/
â”œâ”€â”€ docker-compose.yml    # Container-Konfiguration (whisper-api, ollama, frontend)
â”œâ”€â”€ .env                  # Secrets (HF_TOKEN, ASR_MODEL, OLLAMA_MODEL)
â”œâ”€â”€ .env.example          # Vorlage
â”œâ”€â”€ docs/                 # Detaillierte Dokumentation
â”‚   â”œâ”€â”€ CLAUDE.md         # VollstÃ¤ndige Projektdokumentation
â”‚   â”œâ”€â”€ AI_SUMMARY.md     # Ollama Integration & Zusammenfassungen
â”‚   â”œâ”€â”€ DIARIZATION_FIX.md
â”‚   â”œâ”€â”€ GPU_MEMORY_FIX.md
â”‚   â””â”€â”€ INDEXEDDB_STORAGE.md
â””â”€â”€ frontend/
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ nginx.conf        # Reverse Proxy fÃ¼r /api und /ollama
    â”œâ”€â”€ index.html        # UI mit Tab-Navigation
    â”œâ”€â”€ styles.css        # Styling
    â””â”€â”€ app.js            # Transkriptor + SummaryManager Klassen
```

## ğŸ”’ Sicherheit & Datenschutz

- **100% lokal** â€“ Alle Daten werden auf deinem Server verarbeitet
- **Keine Cloud-Dienste** â€“ WhisperX, Ollama und alle Modelle laufen lokal
- **Keine DatenÃ¼bertragung** â€“ Audio, Transkripte und Zusammenfassungen verlassen nie deinen Server
- **DSGVO-konform** â€“ Ideal fÃ¼r sensible Inhalte (Meetings, Interviews, vertrauliche Aufnahmen)
- **Offline-fÃ¤hig** â€“ Funktioniert ohne Internetverbindung (nach Model-Download)
- **Hugging Face Token** â€“ Wird nur fÃ¼r einmaligen Model-Download verwendet

## ğŸ“„ Lizenz

MIT License â€“ Frei verwendbar fÃ¼r private und kommerzielle Zwecke.

## ğŸ™ Credits

- [WhisperX](https://github.com/m-bain/whisperX) â€“ ASR mit Alignment und Diarization
- [whisper-asr-webservice](https://github.com/ahmetoner/whisper-asr-webservice) â€“ API Backend
- [pyannote-audio](https://github.com/pyannote/pyannote-audio) â€“ Speaker Diarization
- [Ollama](https://ollama.com) â€“ Lokale LLM-Integration fÃ¼r KI-Zusammenfassungen
- [Qwen2.5](https://huggingface.co/Qwen/Qwen2.5-7B) â€“ Exzellentes LLM fÃ¼r strukturierte Zusammenfassungen

## ğŸ“š Weitere Dokumentation

Detaillierte Dokumentation findest du im `docs/` Verzeichnis:

- **[docs/CLAUDE.md](docs/CLAUDE.md)** â€“ VollstÃ¤ndige ProjektÃ¼bersicht und Architektur
- **[docs/AI_SUMMARY.md](docs/AI_SUMMARY.md)** â€“ Ollama Integration, Prompts und Modellvergleiche
- **[docs/INDEXEDDB_STORAGE.md](docs/INDEXEDDB_STORAGE.md)** â€“ Audio-Speicherung mit IndexedDB
- **[docs/GPU_MEMORY_FIX.md](docs/GPU_MEMORY_FIX.md)** â€“ CUDA Memory Troubleshooting
- **[docs/DIARIZATION_FIX.md](docs/DIARIZATION_FIX.md)** â€“ Speaker Diarization Optimierung
